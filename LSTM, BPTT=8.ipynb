{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load the game environment </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld import gameEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADLVJREFUeJzt3X+oX/V9x/Hna4nW1m7VuC1kRmZGRZGBUUOnWMamZrO26P4oopRRhpB/uk3XQqvbX4X90cJo6x+jELSdDOePWl0lFLssTRmDkXr9sVYT00Qba4Ia7XR2Dralfe+Pc9Jdw81ybu73+733+Hk+4PL9nnO+33w/h5PX9/y4577fqSokteUXlnsAkmbP4EsNMvhSgwy+1CCDLzXI4EsNMvhSg5YU/CTXJNmbZH+S2yY1KEnTlZO9gSfJKuAHwGbgIPAYcFNV7Z7c8CRNw+olvPcDwP6qeh4gyX3A9cBxg5/E2wS1JJdeeulyD2FFO3DgAK+99lpO9LqlBP9s4MV50weB31rCvyed0Nzc3HIPYUXbtGnToNctJfiDJNkCbJn250gabinBPwScM296fT/vbapqK7AVPNSXVoqlXNV/DDgvyYYkpwI3Ao9MZliSpumk9/hVdSTJHwPfAlYBX6mqZyY2MklTs6Rz/Kr6JvDNCY1F0ox4557UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UoBMGP8lXkhxO8vS8eWuSbE+yr388c7rDlDRJQ/b4fwNcc8y824AdVXUesKOfljQSJwx+Vf0T8G/HzL4euLt/fjfwBxMel6QpOtlz/LVV9VL//GVg7YTGI2kGltxJp6rq/2uUYScdaeU52T3+K0nWAfSPh4/3wqraWlWbqmpYUy9JU3eywX8E+Hj//OPANyYzHEmzMOTXefcC/wKcn+RgkpuBzwGbk+wDru6nJY3ECc/xq+qm4yy6asJjkTQj3rknNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNWhI6a1zkuxMsjvJM0lu6efbTUcaqSF7/CPAp6rqQuAy4BNJLsRuOtJoDemk81JVPdE//wmwBzgbu+lIo7WohhpJzgUuBnYxsJuODTWklWfwxb0k7wW+DtxaVW/OX1ZVBSzYTceGGtLKMyj4SU6hC/09VfVQP3twNx1JK8uQq/oB7gL2VNUX5i2ym440UkPO8a8A/hD4fpKn+nl/Ttc954G+s84LwA3TGaKkSRvSSeefgRxnsd10pBHyzj2pQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQQZfatCiau5pFhasYCZNlHt8qUEGX2rQkJp7pyX5bpJ/7TvpfLafvyHJriT7k9yf5NTpD1fSJAzZ4/8XcGVVXQRsBK5JchnweeCLVfV+4HXg5ukNU9IkDemkU1X1H/3kKf1PAVcCD/bz7aQjjcjQuvqr+gq7h4HtwHPAG1V1pH/JQbq2Wgu9d0uSuSRzkxiwpKUbFPyq+mlVbQTWAx8ALhj6AXbSkVaeRV3Vr6o3gJ3A5cAZSY7eB7AeODThsUmakiFX9X8lyRn983cDm+k65u4EPtq/zE460ogMuXNvHXB3klV0XxQPVNW2JLuB+5L8JfAkXZstSSMwpJPO9+haYx87/3m6831JI+Ode1KDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDBge/L7H9ZJJt/bSddKSRWswe/xa6IptH2UlHGqmhDTXWAx8G7uyng510pNEausf/EvBp4Gf99FnYSUcarSF19T8CHK6qx0/mA+ykI608Q+rqXwFcl+Ra4DTgl4A76Dvp9Ht9O+lIIzKkW+7tVbW+qs4FbgS+XVUfw0460mgt5ff4nwE+mWQ/3Tm/nXSkkRhyqP9zVfUd4Dv9czvpSCPlnXtSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNGlSII8kB4CfAT4EjVbUpyRrgfuBc4ABwQ1W9Pp1hSpqkxezxf7eqNs6rlnsbsKOqzgN29NOSRmAph/rX0zXSABtqSKMyNPgF/EOSx5Ns6eetraqX+ucvA2snPjpJUzG02OYHq+pQkl8Ftid5dv7CqqoktdAb+y+KLQstk7Q8Bu3xq+pQ/3gYeJiuuu4rSdYB9I+Hj/NeO+lIK8yQFlqnJ/nFo8+B3wOeBh6ha6QBNtSQRmXIof5a4OGuQS6rgb+rqkeTPAY8kORm4AXghukNU9IknTD4feOMixaY/2PgqmkMStJ0eeee1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1KChf52nmclyD2CFW/CPQLVI7vGlBhl8qUEGX2qQwZcaZPClBhl8qUEGX2rQoOAnOSPJg0meTbInyeVJ1iTZnmRf/3jmtAcraTKG7vHvAB6tqgvoynDtwU460mgNqbL7PuC3gbsAquq/q+oN7KQjjdaQPf4G4FXgq0meTHJnX2bbTjrSSA0J/mrgEuDLVXUx8BbHHNZXVXGcm6iTbEkyl2RuqYOVNBlDgn8QOFhVu/rpB+m+COykI43UCYNfVS8DLyY5v591FbAbO+lIozX0z3L/BLgnyanA88Af0X1p2ElHGqFBwa+qp4CFDtXtpCONkHfuSQ0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0aUlf//CRPzft5M8mtdtKRxmtIsc29VbWxqjYClwL/CTyMnXSk0Vrsof5VwHNV9QJ20pFGa7HBvxG4t39uJx1ppAYHvy+tfR3wtWOX2UlHGpfF7PE/BDxRVa/003bSkUZqMcG/if87zAc76UijNSj4fXfczcBD82Z/DticZB9wdT8taQSGdtJ5CzjrmHk/xk460ih5557UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UoKGlt/4syTNJnk5yb5LTkmxIsivJ/iT391V4JY3AkBZaZwN/Cmyqqt8EVtHV1/888MWqej/wOnDzNAcqaXKGHuqvBt6dZDXwHuAl4ErgwX65nXSkERnSO+8Q8FfAj+gC/+/A48AbVXWkf9lB4OxpDVLSZA051D+Trk/eBuDXgNOBa4Z+gJ10pJVnSHntq4EfVtWrAEkeAq4Azkiyut/rrwcOLfTmqtoKbO3fu2CbLUmzNeQc/0fAZUnekyR0tfR3AzuBj/avsZOONCJDzvF30V3EewL4fv+ercBngE8m2U/XbOOuKY5T0gSla3Q7ow/zUF9LNMv/r2O0adMm5ubmcqLXeeee1CCDLzXI4EsNMvhSgwa1yZ6g14C3+sd3il/G9ZmZ7jfKg63odTkJQ9bn14f8QzO9qg+QZK6qNs30Q6fI9Vm53knrApNdHw/1pQYZfKlByxH8rcvwmdPk+qxc76R1gQmuz8zP8SUtPw/1pQbNNPhJrkmyt6/Td9ssP3upkpyTZGeS3X39wVv6+WuSbE+yr388c7nHuhhJViV5Msm2fnq0tRSTnJHkwSTPJtmT5PIxb59p1rqcWfCTrAL+GvgQcCFwU5ILZ/X5E3AE+FRVXQhcBnyiH/9twI6qOg/Y0U+PyS3AnnnTY66leAfwaFVdAFxEt16j3D5Tr3VZVTP5AS4HvjVv+nbg9ll9/hTW5xvAZmAvsK6ftw7Yu9xjW8Q6rKcLw5XANiB0N4isXmibreQf4H3AD+mvW82bP8rtQ1fK7kVgDd2NdtuA35/U9pnlof7RFTlqtHX6kpwLXAzsAtZW1Uv9opeBtcs0rJPxJeDTwM/66bMYby3FDcCrwFf7U5c7k5zOSLdPTbnWpRf3FinJe4GvA7dW1Zvzl1X3NTyKX5Mk+QhwuKoeX+6xTMhq4BLgy1V1Md2t4W87rB/Z9llSrcsTmWXwDwHnzJs+bp2+lSrJKXShv6eqHupnv5JkXb98HXB4uca3SFcA1yU5ANxHd7h/B30txf41Y9pGB4GD1VWMgq5q1CWMd/v8vNZlVf0P8LZal/1rTnr7zDL4jwHn9VclT6W7UPHIDD9/Sfp6g3cBe6rqC/MWPUJXcxBGVHuwqm6vqvVVdS7dtvh2VX2MkdZSrKqXgReTnN/POlobcpTbh2nXupzxBYtrgR8AzwF/sdwXUBY59g/SHSZ+D3iq/7mW7rx4B7AP+EdgzXKP9STW7XeAbf3z3wC+C+wHvga8a7nHt4j12AjM9dvo74Ezx7x9gM8CzwJPA38LvGtS28c796QGeXFPapDBlxpk8KUGGXypQQZfapDBlxpk8KUGGXypQf8L/dW4batFx4MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(partial=True,size=9)\n",
    "prev_state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set Device </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Training Deep Recurrent Q Network (LSTM) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hyper-parameters </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_IMAGE_DIM = 84\n",
    "OUT_SIZE = 4\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEP = 8\n",
    "GAMMA = 0.99\n",
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 0.1\n",
    "TOTAL_EPSIODES = 20000\n",
    "MAX_STEPS = 50\n",
    "MEMORY_SIZE = 3000\n",
    "UPDATE_FREQ = 5\n",
    "PERFORMANCE_SAVE_INTERVAL = 500\n",
    "TARGET_UPDATE_FREQ = 20000 #steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Build Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv_layer1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (conv_layer2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (conv_layer3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer4): Conv2d(64, 512, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (lstm_layer): LSTM(512, 512, batch_first=True)\n",
      "  (adv): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (val): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,out_size):\n",
    "        super(Network,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=8,stride=4) # potential check - in_channels\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=4,stride=2)\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64,out_channels=512,kernel_size=7,stride=1)\n",
    "        self.lstm_layer = nn.LSTM(input_size=512,hidden_size=512,num_layers=1,batch_first=True)\n",
    "        self.adv = nn.Linear(in_features=512,out_features=self.out_size)\n",
    "        self.val = nn.Linear(in_features=512,out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x,bsize,time_step,hidden_state,cell_state):\n",
    "        x = x.view(bsize*time_step,1,self.input_size,self.input_size)\n",
    "        \n",
    "        conv_out = self.conv_layer1(x)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer2(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer3(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer4(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        \n",
    "        conv_out = conv_out.view(bsize,time_step,512)\n",
    "        \n",
    "        lstm_out = self.lstm_layer(conv_out,(hidden_state,cell_state))\n",
    "        out = lstm_out[0][:,time_step-1,:]\n",
    "        h_n = lstm_out[1][0]\n",
    "        c_n = lstm_out[1][1]\n",
    "        \n",
    "        adv_out = self.adv(out)\n",
    "        val_out = self.val(out)\n",
    "        \n",
    "        qout = val_out.expand(bsize,self.out_size) + (adv_out - adv_out.mean(dim=1).unsqueeze(dim=1).expand(bsize,self.out_size))\n",
    "        \n",
    "        return qout, (h_n,c_n)\n",
    "    \n",
    "    def init_hidden_states(self,bsize):\n",
    "        h = torch.zeros(1,bsize,512).float().to(device)\n",
    "        c = torch.zeros(1,bsize,512).float().to(device)\n",
    "        \n",
    "        return h,c\n",
    "    \n",
    "main_model = Network(input_size=INPUT_IMAGE_DIM,out_size=OUT_SIZE).to(device)\n",
    "print(main_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Experience Replay </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \n",
    "    def __init__(self,memsize):\n",
    "        self.memsize = memsize\n",
    "        self.memory = deque(maxlen=self.memsize)\n",
    "    \n",
    "    def add_episode(self,epsiode):\n",
    "        self.memory.append(epsiode)\n",
    "    \n",
    "    def get_batch(self,bsize,time_step):\n",
    "        sampled_epsiodes = random.sample(self.memory,bsize)\n",
    "        batch = []\n",
    "        for episode in sampled_epsiodes:\n",
    "            point = np.random.randint(0,len(episode)+1-time_step)\n",
    "            batch.append(episode[point:point+time_step])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preprocess Image </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    return rgb2gray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd367714080>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess_image(prev_state),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Save Dictionary Function </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('data/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> DQN With LSTM </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populated with 3000 Episodes\n"
     ]
    }
   ],
   "source": [
    "mem = Memory(memsize=MEMORY_SIZE)\n",
    "\n",
    "main_model = Network(input_size=INPUT_IMAGE_DIM,out_size=OUT_SIZE).float().to(device)\n",
    "target_model = Network(input_size=INPUT_IMAGE_DIM,out_size=OUT_SIZE).float().to(device)\n",
    "\n",
    "target_model.load_state_dict(main_model.state_dict())\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(main_model.parameters(),lr=0.00025)\n",
    "\n",
    "\n",
    "# Fill memory\n",
    "for i in range(0,MEMORY_SIZE):\n",
    "    \n",
    "    prev_state = env.reset()\n",
    "    processed_prev_state = preprocess_image(prev_state)\n",
    "    step_count = 0\n",
    "    local_memory = []\n",
    "    \n",
    "    while step_count < MAX_STEPS:\n",
    "        \n",
    "        step_count +=1\n",
    "        action = np.random.randint(0,4)\n",
    "        next_state,reward,done = env.step(action)\n",
    "        processed_next_state = preprocess_image(next_state)\n",
    "        \n",
    "        local_memory.append((processed_prev_state,action,reward,processed_next_state))\n",
    "        \n",
    "        prev_state = next_state\n",
    "        processed_prev_state = processed_next_state\n",
    "    \n",
    "    mem.add_episode(local_memory)\n",
    "        \n",
    "print('Populated with %d Episodes'%(len(mem.memory)))\n",
    "\n",
    "# Start Algorithm\n",
    "epsilon = INITIAL_EPSILON\n",
    "loss_stat = []\n",
    "reward_stat = []\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(0,TOTAL_EPSIODES):\n",
    "    \n",
    "    prev_state = env.reset()\n",
    "    processed_prev_state = preprocess_image(prev_state)\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    local_memory = []\n",
    "\n",
    "    hidden_state, cell_state = main_model.init_hidden_states(bsize=1)\n",
    "    \n",
    "    while step_count < MAX_STEPS:\n",
    "        \n",
    "        step_count +=1\n",
    "        total_steps +=1\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "            model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "            action = np.random.randint(0,4)\n",
    "            hidden_state = model_out[1][0]\n",
    "            cell_state = model_out[1][1]\n",
    "            \n",
    "        else:\n",
    "            torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "            model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "            out = model_out[0]\n",
    "            action = int(torch.argmax(out[0]))\n",
    "            hidden_state = model_out[1][0]\n",
    "            cell_state = model_out[1][1]\n",
    "        \n",
    "        next_state,reward,done = env.step(action)\n",
    "        total_reward += reward\n",
    "        processed_next_state = preprocess_image(next_state)\n",
    "        \n",
    "        local_memory.append((processed_prev_state,action,reward,processed_next_state))\n",
    "        \n",
    "        prev_state = next_state\n",
    "        processed_prev_state = processed_next_state\n",
    "        \n",
    "        \n",
    "        if (total_steps % TARGET_UPDATE_FREQ) == 0:\n",
    "            target_model.load_state_dict(main_model.state_dict())\n",
    "       \n",
    "        if (total_steps % UPDATE_FREQ) == 0:\n",
    "            \n",
    "            hidden_batch, cell_batch = main_model.init_hidden_states(bsize=BATCH_SIZE)\n",
    "            \n",
    "            batch = mem.get_batch(bsize=BATCH_SIZE,time_step=TIME_STEP)\n",
    "            \n",
    "            current_states = []\n",
    "            acts = []\n",
    "            rewards = []\n",
    "            next_states = []\n",
    "            \n",
    "            for b in batch:\n",
    "                cs,ac,rw,ns = [],[],[],[]\n",
    "                for element in b:\n",
    "                    cs.append(element[0])\n",
    "                    ac.append(element[1])\n",
    "                    rw.append(element[2])\n",
    "                    ns.append(element[3])\n",
    "                current_states.append(cs)\n",
    "                acts.append(ac)\n",
    "                rewards.append(rw)\n",
    "                next_states.append(ns)\n",
    "            \n",
    "            current_states = np.array(current_states)\n",
    "            acts = np.array(acts)\n",
    "            rewards = np.array(rewards)\n",
    "            next_states = np.array(next_states)\n",
    "            \n",
    "            torch_current_states = torch.from_numpy(current_states).float().to(device)\n",
    "            torch_acts = torch.from_numpy(acts).long().to(device)\n",
    "            torch_rewards = torch.from_numpy(rewards).float().to(device)\n",
    "            torch_next_states = torch.from_numpy(next_states).float().to(device)\n",
    "            \n",
    "            \n",
    "            Q_next,_ = target_model.forward(torch_next_states,bsize=BATCH_SIZE,time_step=TIME_STEP,hidden_state=hidden_batch,cell_state=cell_batch)\n",
    "            Q_next_max,__ = Q_next.detach().max(dim=1)\n",
    "            target_values = torch_rewards[:,TIME_STEP-1] + (GAMMA * Q_next_max)\n",
    "            \n",
    "            Q_s, _ = main_model.forward(torch_current_states,bsize=BATCH_SIZE,time_step=TIME_STEP,hidden_state=hidden_batch,cell_state=cell_batch)\n",
    "            Q_s_a = Q_s.gather(dim=1,index=torch_acts[:,TIME_STEP-1].unsqueeze(dim=1)).squeeze(dim=1)\n",
    "            \n",
    "            \n",
    "            loss = criterion(Q_s_a,target_values)\n",
    "            \n",
    "            #  save performance measure\n",
    "            loss_stat.append(loss.item())\n",
    "            \n",
    "            # make previous grad zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # update params\n",
    "            optimizer.step()\n",
    "\n",
    "    # save performance measure\n",
    "    reward_stat.append(total_reward)\n",
    "    \n",
    "    mem.add_episode(local_memory)\n",
    "\n",
    "    if epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/TOTAL_EPSIODES\n",
    "    \n",
    "    if (episode + 1)% PERFORMANCE_SAVE_INTERVAL == 0:\n",
    "        perf = {}\n",
    "        perf['loss'] = loss_stat\n",
    "        perf['total_reward'] = reward_stat\n",
    "        save_obj(name='LSTM_POMDP_V4',obj=perf)\n",
    "    \n",
    "    #print('Episode : ',episode+1,'Epsilon : ',epsilon,'Reward : ',total_reward,)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Save Primary Network Weights </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(main_model.state_dict(),'data/LSTM_POMDP_V4_WEIGHTS.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Testing Policy </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load Primary Network Weights </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = torch.load('data/LSTM_POMDP_V4_WEIGHTS.torch')\n",
    "main_model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Testing\n",
    "epsilon = INITIAL_EPSILON\n",
    "FINAL_EPSILON = 0.01\n",
    "TOTAL_EPSIODES = 10000\n",
    "loss_stat = []\n",
    "reward_stat = []\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(0,TOTAL_EPSIODES):\n",
    "    \n",
    "    prev_state = env.reset()\n",
    "    processed_prev_state = preprocess_image(prev_state)\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    hidden_state, cell_state = main_model.init_hidden_states(bsize=1)\n",
    "    \n",
    "    while step_count < MAX_STEPS:\n",
    "        \n",
    "        step_count +=1\n",
    "        total_steps +=1\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "            model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "            action = np.random.randint(0,4)\n",
    "            hidden_state = model_out[1][0]\n",
    "            cell_state = model_out[1][1]\n",
    "            \n",
    "        else:\n",
    "            torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "            model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "            out = model_out[0]\n",
    "            action = int(torch.argmax(out[0]))\n",
    "            hidden_state = model_out[1][0]\n",
    "            cell_state = model_out[1][1]\n",
    "        \n",
    "        next_state,reward,done = env.step(action)\n",
    "        total_reward += reward\n",
    "        processed_next_state = preprocess_image(next_state)\n",
    "        \n",
    "        prev_state = next_state\n",
    "        processed_prev_state = processed_next_state\n",
    "            \n",
    "    # save performance measure\n",
    "    reward_stat.append(total_reward)\n",
    "    \n",
    "    if epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/TOTAL_EPSIODES\n",
    "    \n",
    "    if (episode + 1)% PERFORMANCE_SAVE_INTERVAL == 0:\n",
    "        perf = {}\n",
    "        perf['loss'] = loss_stat\n",
    "        perf['total_reward'] = reward_stat\n",
    "        save_obj(name='LSTM_POMDP_V4_TEST',obj=perf)\n",
    "    \n",
    "    print('Episode : ',episode+1,'Epsilon : ',epsilon,'Reward : ',total_reward)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Create Policy GIF </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Collect Frames Of an Episode Using Trained Network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_env = gameEnv(partial=False,size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward : 3\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "local_frames = []\n",
    "random.seed(110)\n",
    "np.random.seed(110)\n",
    "\n",
    "for episode in range(0,1):\n",
    "    \n",
    "    prev_state = env.reset()\n",
    "    random.seed(110)\n",
    "    np.random.seed(110)\n",
    "    full_env_prev = full_env.reset()\n",
    "    \n",
    "    frames.append(full_env_prev)\n",
    "    local_frames.append(prev_state)\n",
    "    processed_prev_state = preprocess_image(prev_state)\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    hidden_state, cell_state = main_model.init_hidden_states(bsize=1)\n",
    "    \n",
    "    while step_count < MAX_STEPS:\n",
    "        \n",
    "        step_count +=1\n",
    "        \n",
    "        torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "        model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "        out = model_out[0]\n",
    "        action = int(torch.argmax(out[0]))\n",
    "        hidden_state = model_out[1][0]\n",
    "        cell_state = model_out[1][1]\n",
    "        \n",
    "        next_state, reward, d = env.step(action)\n",
    "        full_env_next,r,g = full_env.step(action)\n",
    "        total_reward += reward\n",
    "        frames.append(full_env_next)\n",
    "        local_frames.append(next_state)\n",
    "        processed_next_state = preprocess_image(next_state)\n",
    "        \n",
    "        \n",
    "        prev_state = next_state\n",
    "        processed_prev_state = processed_next_state\n",
    "\n",
    "print('Total Reward : %d'%(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Frames to GIF </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayank/.conda/envs/myenv/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/mayank/.conda/envs/myenv/lib/python3.6/site-packages/ipykernel/__main__.py:8: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "from scipy.misc import imresize\n",
    "resized_frames = []\n",
    "resized_local_frames = []\n",
    "\n",
    "for i in range(0,len(frames)):\n",
    "    resized_frames.append(imresize(frames[i],(256,256)))\n",
    "    resized_local_frames.append(imresize(local_frames[i],(256,256)))\n",
    "\n",
    "imageio.mimsave('data/GIFs/LSTM_SIZE_9_frames.gif',resized_frames,fps=3)\n",
    "imageio.mimsave('data/GIFs/LSTM_SIZE_9_local.gif',resized_local_frames,fps=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
